 GPU Detection & Optimization Rules

## GPU Detection Strategy
```python
@dataclass
class GPUInfo:
    vendor: str  # 'nvidia', 'amd', 'intel', 'apple', 'cpu'
    model: str
    vram_gb: int
    driver_version: str
    compute_capability: Optional[str] = None
    rocm_version: Optional[str] = None
    metal_support: bool = False

class GPUDetector:
    def detect(self) -> GPUInfo:
        # Try in order: NVIDIA -> AMD -> Apple -> Intel -> CPU
        pass

    def _detect_nvidia(self) -> Optional[GPUInfo]:
        # Use nvidia-smi for detection
        # Parse output for model, VRAM, driver version
        pass

    def _detect_amd(self) -> Optional[GPUInfo]:
        # Use rocm-smi for ROCm support
        # Parse lspci for fallback detection
        pass
```

## Model Selection Logic
```python
class ModelOptimizer:
    VRAM_REQUIREMENTS = {
        'codellama:34b': 24,
        'codellama:13b': 12,
        'codellama:7b': 6,
        'stable-code:3b': 3,
        'phi:2.7b': 2
    }

    def get_optimal_config(self, gpu_info: GPUInfo) -> ModelConfig:
        if gpu_info.vram_gb >= 24:
            return self._high_end_config()
        elif gpu_info.vram_gb >= 12:
            return self._mid_range_config()
        elif gpu_info.vram_gb >= 6:
            return self._low_end_config()
        else:
            return self._cpu_config()
```

## Benchmark Implementation
```python
class GPUBenchmark:
    async def benchmark_model(self, model_name: str, gpu_info: GPUInfo) -> BenchmarkResult:
        # Test inference speed
        # Measure memory usage
        # Test context length limits
        # Return performance metrics
        pass

    async def optimize_batch_size(self, model_name: str) -> int:
        # Binary search for optimal batch size
        # Monitor VRAM usage
        # Find sweet spot for throughput
        pass
```

## Hardware-Specific Optimizations
- **NVIDIA**: Use CUDA cores, optimize for tensor cores
- **AMD**: Use ROCm, optimize for compute units
- **Apple**: Use Metal Performance Shaders
- **CPU**: Use multiple cores, optimize for cache

## Configuration Templates
```python
NVIDIA_RTX_4090_CONFIG = {
    'model': 'codellama:34b-instruct',
    'context_length': 16384,
    'batch_size': 8,
    'precision': 'fp16',
    'use_flash_attention': True
}

AMD_RX_7900_XT_CONFIG = {
    'model': 'codellama:13b-instruct',
    'context_length': 8192,
    'batch_size': 6,
    'precision': 'fp16',
    'use_rocm_optimization': True
}
